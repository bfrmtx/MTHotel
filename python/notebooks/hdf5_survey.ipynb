{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81f3eea0",
   "metadata": {},
   "source": [
    "Just a test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a37db1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import h5py\n",
    "\n",
    "###############################################\n",
    "def get_script_directory(n_levels_up=0, explicit_tmp_dir=None):\n",
    "    main_dir = ''\n",
    "    script_dir = ''\n",
    "    include_dir = ''\n",
    "    temp_dir = '/tmp'\n",
    "    try:\n",
    "        # Check if running in a standard Python script\n",
    "        script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "        # return script_dir\n",
    "    except NameError:\n",
    "        # If __file__ is not defined, we are likely in a Jupyter Notebook\n",
    "        from IPython import get_ipython\n",
    "        if 'IPKernelApp' in get_ipython().config:\n",
    "            script_dir = os.getcwd()\n",
    "        else:\n",
    "            raise RuntimeError(\"Unable to determine the script directory\")\n",
    "    # cd up n_levels_up\n",
    "    main_dir = script_dir\n",
    "    for i in range(n_levels_up):\n",
    "        main_dir = os.path.dirname(main_dir)\n",
    "    include_dir = os.path.join(main_dir, 'include')\n",
    "    # leave the temporary directory to the user\n",
    "    if explicit_tmp_dir is not None:\n",
    "        temp_dir = explicit_tmp_dir\n",
    "        return main_dir, script_dir, include_dir, temp_dir\n",
    "    # else we try the simplest way to get the temporary directory\n",
    "    # determine if we run Linux, MacOS or Windows\n",
    "    if sys.platform == 'linux':\n",
    "        temp_dir = '/tmp'\n",
    "    elif sys.platform == 'darwin':\n",
    "        temp_dir = '/tmp'\n",
    "    elif sys.platform == 'win32':\n",
    "        temp_dir = os.environ['TEMP']\n",
    "    else:\n",
    "        raise RuntimeError(\"Unknown platform\")\n",
    "\n",
    "    # return all directories\n",
    "    return main_dir, script_dir, include_dir, temp_dir\n",
    "####################################\n",
    "main_dir, script_dir, include_dir, temp_dir = get_script_directory(1)\n",
    "data_dir = os.path.join(main_dir, 'data')\n",
    "# add the include directory to the path\n",
    "sys.path.append(include_dir)\n",
    "import atss_file as atss\n",
    "import atss_extended as atss_ext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760a8d60",
   "metadata": {},
   "source": [
    "## Here we go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92ce2670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading  5  files with  7912 samples each\n",
      "array shape:  (7912,)\n"
     ]
    }
   ],
   "source": [
    "# read all JSON files in the run directory\n",
    "survey = 'Northern Mining'\n",
    "site = 'Sarıçam'\n",
    "run = '006'\n",
    "run_dir = os.path.join(data_dir, 'Sarıçam', 'run_006')\n",
    "channels = []\n",
    "ts = []\n",
    "files = []\n",
    "for file in os.listdir(run_dir):\n",
    "    if file.endswith('.json'):\n",
    "        with open(os.path.join(run_dir, file)) as f:\n",
    "            # get the file name without the extension from f\n",
    "            files.append(os.path.splitext(f.name)[0])\n",
    "            channels.append(atss.read_header(f.name))\n",
    "            # in the real world you may read chunks of data\n",
    "            ts.append(atss.read_data(f.name))\n",
    "\n",
    "print(\"reading \", len(channels) , \" files with \", atss.samples(files[0]), \"samples each\" )\n",
    "print(\"array shape: \", ts[0].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c6792fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the file if it exists\n",
    "if os.path.exists('./demo_surveys.h5'):\n",
    "    os.remove('./demo_surveys.h5')\n",
    "with h5py.File('./demo_surveys.h5', 'w') as hdf:\n",
    "        su = hdf.create_group(survey)\n",
    "        cal = hdf.create_group(survey + '/calibration')\n",
    "        s1 = hdf.create_group(su.name + '/' + site)\n",
    "        # take a run - so we can later add data from other sensors / transmitters here\n",
    "        r1 = hdf.create_group(s1.name + '/' + run)\n",
    "        # loop over the channels and ts\n",
    "        for i in range(len(channels)):\n",
    "            header = channels[i]\n",
    "            #data = ts[i]\n",
    "            # create a group for each channel\n",
    "            ch = r1.create_group(header['channel_type'])\n",
    "            # create the datasets for the data ts[i]\n",
    "            ds = ch.create_dataset('data', data = ts[i])\n",
    "            # ds = ch.create_dataset('data', data = ts[i])\n",
    "            # add the attributes to the dataset which are key-value pairs from the header\n",
    "            #ds.attrs['channel_type'] = header['channel_type']\n",
    "            #ds.attrs['serial'] = header['serial']\n",
    "            # add the attributes to the group which are key-value pairs from the header\n",
    "            \n",
    "            for key in header.keys():\n",
    "                # skip / ignore all \"sensor_calibration\" key\n",
    "                if key == \"sensor_calibration\":\n",
    "                     continue\n",
    "                else:\n",
    "                    try:\n",
    "                        ch.attrs[key] = header[key]\n",
    "                    except:\n",
    "                        print('error adding attribute', key, header[key])   \n",
    "\n",
    "        # close the file\n",
    "        hdf.close()    \n",
    "        \n",
    "     \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2485bcb8",
   "metadata": {},
   "source": [
    "try to read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7383366d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (3874403866.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[16], line 9\u001b[0;36m\u001b[0m\n\u001b[0;31m    get = hdf.get(survey + '/' + site + '/' + run + '/' view_channel)\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "view_channel = 'Hx'\n",
    "with h5py.File('./demo_surveys.h5', 'r') as hdf:\n",
    "   all_h5_objs = []\n",
    "   hdf.visit(all_h5_objs.append)\n",
    "   all_groups   = [ obj for obj in all_h5_objs if isinstance(hdf[obj],h5py.Group) ]\n",
    "   all_datasets = [ obj for obj in all_h5_objs if isinstance(hdf[obj],h5py.Dataset) ]\n",
    "   # print(all_groups)\n",
    "   # print(all_datasets)  # need some more time for fishing\n",
    "   str = survey + '/' + site + '/' + run + '/' + view_channel\n",
    "   get = hdf.get(str)\n",
    "   # print the attributes of the dataset as key-value pairs\n",
    "   for key in get.attrs.keys():\n",
    "       print(key, get.attrs[key])\n",
    "   # get the data from the dataset data\n",
    "   hx = get['data']\n",
    "   print(\"getting: \", len(hx), \" samples\")\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3bdb85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
